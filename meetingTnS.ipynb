{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKN5cYeZX5vU"
      },
      "outputs": [],
      "source": [
        "#programming language/compiler to optimize code for GPU inside Python\n",
        "!pip install triton\n",
        "\n",
        "#faster implementation of OpenAI Whisper model\n",
        "!pip install faster-whisper\n",
        "\n",
        "# installs hugging face to download and run Mistral\n",
        "# installs accelerate for an efficient use of the hardware\n",
        "# installs bitsanbytes to allow a big model run in the free Colab RAM\n",
        "!pip install transformers==4.41.2 accelerate==0.30.1 bitsandbytes --upgrade\n",
        "\n",
        "# dependency fix forcing SymPy to be upgraded\n",
        "!pip install --upgrade sympy\n",
        "\n",
        "# converts MP3 and M4A into raw audio files\n",
        "!apt install ffmpeg\n",
        "\n",
        "# connects Google Drive to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os #file manager\n",
        "import subprocess\n",
        "import gc #memory cleaner\n",
        "import torch\n",
        "from faster_whisper import WhisperModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "\n",
        "# CONFIGURATION\n",
        "\n",
        "#folder location\n",
        "folder_path = \"/content/drive/MyDrive/transcriber\"\n",
        "\n",
        "#helper function. prevents gpu to run out of memory\n",
        "#deletes used model in order to download the other model\n",
        "def clean_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# TRANSCRIPTION\n",
        "print(\"\\nStarting Transcription...\")\n",
        "\n",
        "# loads the whisper model'large-v3' for maximum accuracy on financial terms\n",
        "model = WhisperModel(\"large-v3\", device=\"cuda\", compute_type=\"float16\")\n",
        "\n",
        "# find the files\n",
        "all_files = os.listdir(folder_path)\n",
        "media_extensions = (\".m4a\", \".mp3\", \".wav\", \".mp4\", \".mkv\")\n",
        "# uses list comprehension to filter files\n",
        "recordings = [f for f in all_files if f.lower().endswith(media_extensions)]\n",
        "\n",
        "files_to_summarize = []\n",
        "\n",
        "for filename in recordings:\n",
        "    input_path = os.path.join(folder_path, filename)\n",
        "\n",
        "    text_filename = os.path.splitext(filename)[0] + \".txt\"\n",
        "    output_path = os.path.join(folder_path, text_filename)\n",
        "\n",
        "    files_to_summarize.append(output_path)\n",
        "\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"⏭️  Skipping transcription for {filename} (Already exists)\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nTranscribing: {filename}...\")\n",
        "\n",
        "    # convert to clean WAV (fixes \"malformed\" errors)\n",
        "    temp_wav = os.path.join(folder_path, \"temp_processing.wav\")\n",
        "    subprocess.run([\n",
        "        \"ffmpeg\", \"-y\", \"-i\", input_path,\n",
        "        \"-ar\", \"16000\", \"-ac\", \"1\", \"-c:a\", \"pcm_s16le\",\n",
        "        temp_wav\n",
        "    ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    # transcribe\n",
        "    try:\n",
        "        segments, info = model.transcribe(temp_wav, beam_size=5)\n",
        "\n",
        "        print(f\"writing transcript...\")\n",
        "\n",
        "        with open(output_path, \"w\") as f:\n",
        "            for segment in segments:\n",
        "                line = f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\"\n",
        "                f.write(line + \"\\n\")\n",
        "                print(\"|\", end=\"\", flush=True) # progress\n",
        "\n",
        "        print(f\"\\ntranscript saved: {text_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"error transcribing {filename}: {e}\")\n",
        "\n",
        "    # cleanup temp file\n",
        "    if os.path.exists(temp_wav):\n",
        "        os.remove(temp_wav)\n",
        "\n",
        "# delete the whisper to make room for the summary model\n",
        "print(\"\\n unloading whisper to free up memory\")\n",
        "del model\n",
        "clean_memory()\n",
        "\n",
        "\n",
        "# SUMMARIZATION\n",
        "print(\"\\n Starting Summarization...\")\n",
        "\n",
        "# load Mistral 7B using 4-bit quantization to fit in the free GPU\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Mistral-7B-Instruct-v0.2 because it is powerful and ungated model\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "try:\n",
        "  #translate english words to tokens using model's 'dictionary'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    #smart loading the model\n",
        "    summary_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    text_generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=summary_model,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    for txt_path in files_to_summarize:\n",
        "        summary_path = txt_path.replace(\".txt\", \"_SUMMARY.txt\")\n",
        "\n",
        "        if os.path.exists(summary_path):\n",
        "            print(f\" Skipping summary for {os.path.basename(txt_path)}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n Generating Summary for: {os.path.basename(txt_path)}\")\n",
        "\n",
        "        # read the full transcript\n",
        "        with open(txt_path, \"r\") as f:\n",
        "            full_text = f.read()\n",
        "            # limit to first 25,000 characters to fit context window if file is massive\n",
        "            full_text = full_text[:25000]\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "Act as an expert minute-taker. Analyze the transcript and produce a structured report including:\n",
        "- Summary: A narrative overview of the meeting's progression (start, middle, and end).\n",
        "- Tone: The general atmosphere or sentiment.\n",
        "- Key Topics: Bulleted details of primary discussions.\n",
        "- Action Items: Tasks assigned, owners, and due dates.\n",
        "- Decisions: Final agreements and approvals.\n",
        "\n",
        "TRANSCRIPT:\n",
        "{full_text}\n",
        "[/INST]\"\"\"\n",
        "\n",
        "        # settings of the prompt\n",
        "        sequences = text_generator(\n",
        "            prompt,\n",
        "            do_sample=True,\n",
        "            max_new_tokens=1000,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            num_return_sequences=1,\n",
        "        )\n",
        "\n",
        "        # extract the summary text\n",
        "        summary_text = sequences[0]['generated_text']\n",
        "        # clean up the prompt from the output\n",
        "        summary_only = summary_text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "        # save\n",
        "        with open(summary_path, \"w\") as f:\n",
        "            f.write(summary_only)\n",
        "\n",
        "        print(f\"Summary Saved: {os.path.basename(summary_path)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in summarization: {e}\")\n",
        "\n",
        "print(\"\\nCheck Drive folder for .txt and _SUMMARY.txt files.\")"
      ],
      "metadata": {
        "id": "VKoQNgPQYiT8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}